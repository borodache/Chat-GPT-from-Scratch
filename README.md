I have implemented GPT2 from scratch - according to the book: https://www.manning.com/books/build-a-large-language-model-from-scratch

From preproccesing and preparing the data, to implemnting different types of tokenizers, attentions, trasformers and the whole GPT architecture.

The files are "dirty" and some of them weren't cleaned. Also I had left a lot of comments which I used along the road to verify my code results. 
